import logging
import asyncio
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
import random # Ø¨Ø±Ø§ÛŒ ØªØ£Ø®ÛŒØ± ØªØµØ§Ø¯ÙÛŒ
import re # Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² regex
from playwright.async_api import async_playwright, Page, BrowserContext, TimeoutError
import os
import hashlib
import time

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
logging.basicConfig(
    level=logging.INFO, # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ logging.DEBUG ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ITADSource:
    BASE_DEALS_URL = "https://isthereanydeal.com/deals/#filter:N4IgDgTglgxgpiAXKAtlAdk9BXANrgGhBQEMAPJABgF8iAXATzAUQG0BGAXWqA=="
    
    def __init__(self, cache_dir: str = "cache", cache_ttl: int = 3600):
        self.cache_dir = os.path.join(cache_dir, "itad")
        self.cache_ttl = cache_ttl # Ø²Ù…Ø§Ù† Ø²Ù†Ø¯Ú¯ÛŒ Ú©Ø´ Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡ (Ù…Ø«Ù„Ø§Ù‹ 3600 Ø«Ø§Ù†ÛŒÙ‡ = 1 Ø³Ø§Ø¹Øª)
        os.makedirs(self.cache_dir, exist_ok=True)
        logger.info(f"Ù†Ù…ÙˆÙ†Ù‡ ITADSource Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯. Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ú©Ø´: {self.cache_dir}, TTL: {self.cache_ttl} Ø«Ø§Ù†ÛŒÙ‡.")

    def _get_cache_path(self, url: str) -> str:
        """Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ú©Ø´ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ø´ URL ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        url_hash = hashlib.sha256(url.encode('utf-8')).hexdigest()
        return os.path.join(self.cache_dir, f"{url_hash}.html")

    def _is_cache_valid(self, cache_path: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ ÙØ§ÛŒÙ„ Ú©Ø´ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ùˆ Ù…Ù†Ù‚Ø¶ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª."""
        if not os.path.exists(cache_path):
            return False
        # Ø¨Ø±Ø±Ø³ÛŒ TTL
        file_mod_time = os.path.getmtime(cache_path)
        if (time.time() - file_mod_time) > self.cache_ttl:
            logger.debug(f"ÙØ§ÛŒÙ„ Ú©Ø´ {cache_path} Ù…Ù†Ù‚Ø¶ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
            return False
        logger.debug(f"ÙØ§ÛŒÙ„ Ú©Ø´ {cache_path} Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.")
        return True

    async def _parse_deal_element(self, deal_tag: BeautifulSoup) -> Optional[Dict[str, Any]]:
        try:
            title_tag = deal_tag.select_one('h3.game-title a')
            title = title_tag.get_text(strip=True) if title_tag else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
            main_link = title_tag['href'] if title_tag and 'href' in title_tag.attrs else "#"
            logger.debug(f"Ø¯Ø± Ø­Ø§Ù„ ØªØ¬Ø²ÛŒÙ‡ Ø¹Ù†ØµØ± Ù…Ø¹Ø§Ù…Ù„Ù‡: Ø¹Ù†ÙˆØ§Ù†='{title}', Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ='{main_link}'")

            store_tag = deal_tag.select_one('div.deal-store a')
            store = store_tag.get_text(strip=True) if store_tag else "Ù†Ø§Ù…Ø´Ø®Øµ"
            deal_url = store_tag['href'] if store_tag and 'href' in store_tag.attrs else main_link
            logger.debug(f"ÙØ±ÙˆØ´Ú¯Ø§Ù‡='{store}', Ù„ÛŒÙ†Ú© Ù…Ø¹Ø§Ù…Ù„Ù‡='{deal_url}'")

            is_free = False
            discount_text = None

            cut_tag = deal_tag.select_one('div.deal-cut')
            if cut_tag:
                cut_text = cut_tag.get_text(strip=True).lower()
                if "100% off" in cut_text or "free" in cut_text:
                    is_free = True
                    discount_text = "100% Off / Free"
                    logger.debug(f"Ø¨Ø§Ø²ÛŒ '{title}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ÛŒÚ¯Ø§Ù† Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯ (Ù…ØªÙ†: '{cut_text}')")
                else:
                    discount_match = re.search(r'(\d+% off)', cut_text)
                    if discount_match:
                        discount_text = discount_match.group(1)
                        is_free = False
                        logger.debug(f"Ø¨Ø§Ø²ÛŒ '{title}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯: {discount_text}")
                    else:
                        discount_text = "ØªØ®ÙÛŒÙ"
                        is_free = False
                        logger.debug(f"Ø¨Ø§Ø²ÛŒ '{title}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± (Ù†Ø§Ù…Ø´Ø®Øµ) Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯.")
            else:
                logger.debug(f"ØªÚ¯ 'deal-cut' Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ÛŒ '{title}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØºÛŒØ±Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                is_free = False

            return {
                "title": title,
                "store": store,
                "url": deal_url,
                "id_in_db": main_link,
                "is_free": is_free,
                "discount_text": discount_text
            }
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ¬Ø²ÛŒÙ‡ Ø¹Ù†ØµØ± Ù…Ø¹Ø§Ù…Ù„Ù‡ ITAD Ø¨Ø±Ø§ÛŒ ØªÚ¯: {deal_tag.prettify()[:200]}... Ø¯Ù„ÛŒÙ„: {e}", exc_info=True)
            return None

    async def fetch_free_games(self) -> List[Dict[str, Any]]:
        logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø² ØµÙØ­Ù‡ Deals Ø³Ø§ÛŒØª ITAD Ø¨Ø§ Playwright...")
        free_games_list = []
        processed_ids = set()

        cache_path = self._get_cache_path(self.BASE_DEALS_URL)
        html_content = None

        if self._is_cache_valid(cache_path):
            logger.info(f"âœ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ ITAD Ø§Ø² Ú©Ø´: {cache_path}")
            with open(cache_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
        else:
            logger.info(f"Ú©Ø´ ITAD Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª ÛŒØ§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ø¯Ø± Ø­Ø§Ù„ ÙˆØ§Ú©Ø´ÛŒ Ø§Ø² ÙˆØ¨â€ŒØ³Ø§ÛŒØª.")
            browser = None
            try:
                async with async_playwright() as p:
                    browser = await p.chromium.launch(headless=True) 
                    page = await browser.new_page()

                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            logger.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ Deals: {self.BASE_DEALS_URL} (ØªÙ„Ø§Ø´ {attempt + 1}/{max_retries})")
                            if hasattr(page, 'wait_for_loadstate'):
                                await page.goto(self.BASE_DEALS_URL, wait_until='networkidle', timeout=60000)
                                logger.debug("ØµÙØ­Ù‡ ITAD Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² wait_for_loadstate Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
                            else:
                                await page.goto(self.BASE_DEALS_URL, timeout=60000)
                                await asyncio.sleep(5)
                                logger.warning("âš ï¸ 'wait_for_loadstate' Ø¯Ø± Playwright ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§Ø² ØªØ£Ø®ÛŒØ± Ø«Ø§Ø¨Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Playwright Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯.")
                            break
                        except TimeoutError:
                            logger.warning(f"âš ï¸ Timeout Ù‡Ù†Ú¯Ø§Ù… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ ITAD (ØªÙ„Ø§Ø´ {attempt + 1}/{max_retries}).")
                            if attempt < max_retries - 1:
                                retry_delay = 2 ** attempt + random.uniform(0, 2)
                                logger.info(f"Ø¯Ø± Ø­Ø§Ù„ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¯Ø± {retry_delay:.2f} Ø«Ø§Ù†ÛŒÙ‡...")
                                await asyncio.sleep(retry_delay)
                            else:
                                logger.critical(f"ğŸ”¥ ØªÙ…Ø§Ù… ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ ITAD Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Timeout Ø¨Ø§ Ø´Ú©Ø³Øª Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯.")
                                return []
                        except Exception as e:
                            logger.critical(f"ğŸ”¥ Ø®Ø·Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø´Ø¯Ù‡ Ù‡Ù†Ú¯Ø§Ù… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ ITAD (ØªÙ„Ø§Ø´ {attempt + 1}/{max_retries}): {e}", exc_info=True)
                            return []
                    else:
                        logger.critical(f"ğŸ”¥ ØªÙ…Ø§Ù… {max_retries} ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ ITAD Ø¨Ø§ Ø´Ú©Ø³Øª Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯.")
                        return []

                    try:
                        await page.wait_for_selector('article.deal', state='visible', timeout=30000) 
                        logger.debug("Ø§ÙˆÙ„ÛŒÙ† Ø¹Ù†ØµØ± 'article.deal' Ø¯Ø± ØµÙØ­Ù‡ ITAD ÛŒØ§ÙØª Ø´Ø¯.")
                    except TimeoutError:
                        logger.warning("âš ï¸ Ù‡ÛŒÚ† Ø¹Ù†ØµØ± 'article.deal' Ø¯Ø± Ø²Ù…Ø§Ù† Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ (30 Ø«Ø§Ù†ÛŒÙ‡) ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ø­ØªÙˆØ§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ ÛŒØ§ Ø³Ù„Ú©ØªÙˆØ± HTML ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯.")

                    previous_height = -1
                    scroll_attempts = 0
                    max_scroll_attempts = 15
                    scroll_pause_time = random.uniform(1.5, 3)

                    while scroll_attempts < max_scroll_attempts:
                        current_scroll_height = await page.evaluate("document.body.scrollHeight")
                        if current_scroll_height == previous_height:
                            logger.info("Ù¾Ø§ÛŒØ§Ù† Ø§Ø³Ú©Ø±ÙˆÙ„: Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯.")
                            break
                        
                        previous_height = current_scroll_height
                        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                        if hasattr(page, 'wait_for_loadstate'):
                            await page.wait_for_loadstate('networkidle', timeout=10000) 
                        await asyncio.sleep(scroll_pause_time)

                        scroll_attempts += 1
                        logger.debug(f"Ø§Ø³Ú©Ø±ÙˆÙ„ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯. Ø§Ø±ØªÙØ§Ø¹ Ø¬Ø¯ÛŒØ¯: {current_scroll_height} Ù¾ÛŒÚ©Ø³Ù„. ØªÙ„Ø§Ø´: {scroll_attempts}")
                    else:
                        logger.warning(f"âš ï¸ Ø¨Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÙˆÙ„ ({max_scroll_attempts}) Ø±Ø³ÛŒØ¯ÛŒÙ…. Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªÙ…Ø§Ù… Ù…Ø­ØªÙˆØ§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯.")

                    html_content = await page.content()
                    logger.debug(f"Ù…Ø­ØªÙˆØ§ÛŒ HTML Ø±Ù†Ø¯Ø± Ø´Ø¯Ù‡ (Ø¨Ø®Ø´ÛŒ): {html_content[:500]}...")
                    
                    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ ØªØ§Ø²Ù‡ ÙˆØ§Ú©Ø´ÛŒ Ø´Ø¯Ù‡ Ø¯Ø± Ú©Ø´
                    with open(cache_path, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    logger.info(f"âœ… Ù…Ø­ØªÙˆØ§ÛŒ ITAD Ø¯Ø± Ú©Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {cache_path}")

            except Exception as e:
                logger.critical(f"ğŸ”¥ ÛŒÚ© Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø´Ø¯Ù‡ Ø¯Ø± Ù…Ø§Ú˜ÙˆÙ„ ITAD (Ø§Ø³Ú©Ø±Ù¾ÛŒÙ†Ú¯ Playwright) Ø±Ø® Ø¯Ø§Ø¯: {e}", exc_info=True)
                return []
            finally:
                if browser:
                    await browser.close()
                    logger.debug("Ù…Ø±ÙˆØ±Ú¯Ø± Playwright Ø¨Ø³ØªÙ‡ Ø´Ø¯.")

        if not html_content:
            logger.error("âŒ Ù…Ø­ØªÙˆØ§ÛŒ HTML Ø§Ø² ITAD (Ú©Ø´ ÛŒØ§ ÙˆØ§Ú©Ø´ÛŒ) Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")
            return []

        soup = BeautifulSoup(html_content, 'html.parser')
        deal_elements = soup.select('article.deal')

        if not deal_elements:
            logger.warning("âš ï¸ Ù‡ÛŒÚ† Ø¹Ù†ØµØ± 'article.deal' Ø¯Ø± HTML Ø±Ù†Ø¯Ø± Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ ØªØºÛŒÛŒØ± Ø¯Ø± Ø³Ø§Ø®ØªØ§Ø± Ø³Ø§ÛŒØª ÛŒØ§ Ø¹Ø¯Ù… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµØ­ÛŒØ­ Ù…Ø­ØªÙˆØ§ Ø¨Ø§Ø´Ø¯.")
            return []

        logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø¹Ù†Ø§ØµØ± 'article.deal' ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(deal_elements)}")

        for i, deal_tag in enumerate(deal_elements):
            normalized_game = await self._parse_deal_element(deal_tag)
            
            if normalized_game:
                if normalized_game['id_in_db'] not in processed_ids:
                    if normalized_game['is_free']:
                        free_games_list.append(normalized_game)
                        processed_ids.add(normalized_game['id_in_db'])
                        logger.info(f"âœ… Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø² ITAD ÛŒØ§ÙØª Ø´Ø¯: {normalized_game['title']} (ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {normalized_game['store']})")
                    else:
                        free_games_list.append(normalized_game)
                        processed_ids.add(normalized_game['id_in_db'])
                        logger.info(f"ğŸ” Ø¨Ø§Ø²ÛŒ ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ø§Ø² ITAD ÛŒØ§ÙØª Ø´Ø¯: {normalized_game['title']} (ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {normalized_game['store']}, ØªØ®ÙÛŒÙ: {normalized_game['discount_text']})")
                else:
                    logger.debug(f"â„¹ï¸ Ø¨Ø§Ø²ÛŒ '{normalized_game['title']}' Ø§Ø² ITAD Ù‚Ø¨Ù„Ø§Ù‹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ (ID: {normalized_game['id_in_db']}).")
            else:
                logger.warning(f"âš ï¸ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù†ØµØ± Ù…Ø¹Ø§Ù…Ù„Ù‡ ITAD Ø´Ù…Ø§Ø±Ù‡ {i+1} Ø¨Ø§ Ø´Ú©Ø³Øª Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯. Ø§ÛŒÙ† Ø¢ÛŒØªÙ… Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

        if not free_games_list:
            logger.info("â„¹ï¸ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† ÛŒØ§ ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ø¬Ø¯ÛŒØ¯ÛŒ Ø¯Ø± ØµÙØ­Ù‡ Deals Ø³Ø§ÛŒØª ITAD ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            
        return free_games_list
