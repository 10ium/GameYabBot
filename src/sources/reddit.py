import logging
import asyncio
from typing import List, Dict, Any, Optional
import aiohttp
import xml.etree.ElementTree as ET
import re
from bs4 import BeautifulSoup
import hashlib
import random # Ø¨Ø±Ø§ÛŒ ØªØ£Ø®ÛŒØ± ØªØµØ§Ø¯ÙÛŒ
import os
import time # Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø²Ù…Ø§Ù† ÙØ§ÛŒÙ„ Ú©Ø´
import utils.clean_title_for_search as title_cleaner # <--- Ø®Ø· Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡: ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ù…Ø§Ú˜ÙˆÙ„ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† title_cleaner
from utils.store_detector import infer_store_from_game_data # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† ØªØ§Ø¨Ø¹ Ø§Ø² Ù…Ø§Ú˜ÙˆÙ„ Ø¬Ø¯ÛŒØ¯

logging.basicConfig(
    level=logging.INFO, # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ logging.DEBUG ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

class RedditSource:
    def __init__(self, cache_dir: str = "cache", cache_ttl: int = 3600): # TTL Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 1 Ø³Ø§Ø¹Øª
        self.subreddits = [
            'GameDeals',
            'FreeGameFindings',
            'googleplaydeals',
            'AppHookup'
        ]
        self.rss_urls = {sub: f"https://www.reddit.com/r/{sub}/new/.rss" for sub in self.subreddits}
        self.HEADERS = {'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'} # User-Agent Ø¹Ù…ÙˆÙ…ÛŒâ€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø±Ø¯ÛŒØª
        
        self.cache_dir = os.path.join(cache_dir, "reddit")
        self.cache_ttl = cache_ttl
        os.makedirs(self.cache_dir, exist_ok=True)
        logger.info(f"[RedditSource] Ù†Ù…ÙˆÙ†Ù‡ RedditSource Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯. Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ú©Ø´: {self.cache_dir}, TTL: {self.cache_ttl} Ø«Ø§Ù†ÛŒÙ‡.")

    @staticmethod
    def _generate_unique_id(base_id: str, item_url: str) -> str:
        """
        ÛŒÚ© Ø´Ù†Ø§Ø³Ù‡ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ù†Ø§Ø³Ù‡ Ø§ØµÙ„ÛŒ Ùˆ URL Ø¢ÛŒØªÙ… Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        combined_string = f"{base_id}-{item_url}"
        return hashlib.sha256(combined_string.encode()).hexdigest()

    def _get_cache_path(self, url: str) -> str:
        """Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ú©Ø´ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ø´ URL ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        url_hash = hashlib.sha256(url.encode('utf-8')).hexdigest()
        return os.path.join(self.cache_dir, f"{url_hash}.html") # Ú©Ø´ RSS Ùˆ Permalink Ù‡Ø± Ø¯Ùˆ HTML Ù‡Ø³ØªÙ†Ø¯

    def _is_cache_valid(self, cache_path: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ ÙØ§ÛŒÙ„ Ú©Ø´ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ùˆ Ù…Ù†Ù‚Ø¶ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª."""
        if not os.path.exists(cache_path):
            return False
        file_mod_time = os.path.getmtime(cache_path)
        if (time.time() - file_mod_time) > self.cache_ttl:
            logger.debug(f"[RedditSource - _is_cache_valid] ÙØ§ÛŒÙ„ Ú©Ø´ {cache_path} Ù…Ù†Ù‚Ø¶ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
            return False
        logger.debug(f"[RedditSource - _is_cache_valid] ÙØ§ÛŒÙ„ Ú©Ø´ {cache_path} Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.")
        return True

    async def _fetch_with_retry(self, session: aiohttp.ClientSession, url: str, max_retries: int = 3, initial_delay: float = 2) -> Optional[str]:
        """
        ÛŒÚ© URL Ø±Ø§ Ø¨Ø§ Ù…Ú©Ø§Ù†ÛŒØ²Ù… retry Ùˆ exponential backoff ÙˆØ§Ú©Ø´ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ HTML Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        """
        cache_path = self._get_cache_path(url)

        if self._is_cache_valid(cache_path):
            logger.info(f"âœ… [RedditSource - _fetch_with_retry] Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ Ø§Ø² Ú©Ø´: {cache_path}")
            with open(cache_path, 'r', encoding='utf-8') as f:
                return f.read()

        logger.debug(f"[RedditSource - _fetch_with_retry] Ú©Ø´ Ø¨Ø±Ø§ÛŒ {url} Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª ÛŒØ§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ø¯Ø± Ø­Ø§Ù„ ÙˆØ§Ú©Ø´ÛŒ Ø§Ø² ÙˆØ¨â€ŒØ³Ø§ÛŒØª.")
        for attempt in range(max_retries):
            try:
                current_delay = initial_delay * (2 ** attempt) + random.uniform(0, 1) # Exponential backoff + jitter
                logger.debug(f"[RedditSource - _fetch_with_retry] ØªÙ„Ø§Ø´ {attempt + 1}/{max_retries} Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú©Ø´ÛŒ URL: {url} (ØªØ£Ø®ÛŒØ±: {current_delay:.2f} Ø«Ø§Ù†ÛŒÙ‡)")
                await asyncio.sleep(current_delay)
                async with session.get(url, headers=self.HEADERS, timeout=20) as response: # Ø§ÙØ²Ø§ÛŒØ´ timeout Ø¨Ø±Ø§ÛŒ Ø±Ø¯ÛŒØª
                    response.raise_for_status()
                    html_content = await response.text()
                    
                    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
                    with open(cache_path, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    logger.info(f"âœ… [RedditSource - _fetch_with_retry] Ù…Ø­ØªÙˆØ§ Ø¯Ø± Ú©Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {cache_path}")
                    return html_content
            except aiohttp.ClientResponseError as e:
                logger.error(f"âŒ [RedditSource - _fetch_with_retry] Ø®Ø·Ø§ÛŒ HTTP Ù‡Ù†Ú¯Ø§Ù… ÙˆØ§Ú©Ø´ÛŒ {url} (ØªÙ„Ø§Ø´ {attempt + 1}/{max_retries}): Status {e.status}, Message: '{e.message}'", exc_info=True)
                if attempt < max_retries - 1 and e.status in [403, 429, 500, 502, 503, 504]: # Retry on specific error codes
                    logger.info(f"[RedditSource - _fetch_with_retry] Ø¯Ø± Ø­Ø§Ù„ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¨Ø±Ø§ÛŒ {url}...")
                else:
                    logger.critical(f"ğŸ”¥ [RedditSource - _fetch_with_retry] ØªÙ…Ø§Ù… ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú©Ø´ÛŒ {url} Ø¨Ø§ Ø´Ú©Ø³Øª Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯. (Ø¢Ø®Ø±ÛŒÙ† Ø®Ø·Ø§: {e.status})")
                    return None
            except asyncio.TimeoutError:
                logger.error(f"âŒ [RedditSource - _fetch_with_retry] Ø®Ø·Ø§ÛŒ Timeout Ù‡Ù†Ú¯Ø§Ù… ÙˆØ§Ú©Ø´ÛŒ {url} (ØªÙ„Ø§Ø´ {attempt + 1}/{max_retries}).")
                if attempt < max_retries - 1:
                    logger.info(f"[RedditSource - _fetch_with_retry] Ø¯Ø± Ø­Ø§Ù„ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¨Ø±Ø§ÛŒ {url}...")
                else:
                    logger.critical(f"ğŸ”¥ [RedditSource - _fetch_with_retry] ØªÙ…Ø§Ù… ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú©Ø´ÛŒ {url} Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Timeout Ø¨Ø§ Ø´Ú©Ø³Øª Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯.")
                    return None
            except Exception as e:
                logger.critical(f"ğŸ”¥ [RedditSource - _fetch_with_retry] Ø®Ø·Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø´Ø¯Ù‡ Ù‡Ù†Ú¯Ø§Ù… ÙˆØ§Ú©Ø´ÛŒ {url} (ØªÙ„Ø§Ø´ {attempt + 1}/{max_retries}): {e}", exc_info=True)
                return None
        return None # Ø§Ú¯Ø± ØªÙ…Ø§Ù… ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø¨Ø§ Ø´Ú©Ø³Øª Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯

    async def _fetch_and_parse_reddit_permalink(self, session: aiohttp.ClientSession, permalink_url: str) -> Optional[str]:
        """
        ÛŒÚ© Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª Ø±Ø§ ÙˆØ§Ú©Ø´ÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ Ø§ÙˆÙ„ÛŒÙ† Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ù…Ø¹ØªØ¨Ø± Ø±Ø§ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        logger.info(f"[RedditSource - _fetch_and_parse_reddit_permalink] Ø¯Ø± Ø­Ø§Ù„ ÙˆØ§Ú©Ø´ÛŒ Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ: {permalink_url}")
        html_content = await self._fetch_with_retry(session, permalink_url)
        if not html_content:
            logger.warning(f"âš ï¸ [RedditSource - _fetch_and_parse_reddit_permalink] ÙˆØ§Ú©Ø´ÛŒ Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª '{permalink_url}' Ø¨Ø§ Ø´Ú©Ø³Øª Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯. Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            return None

        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† div Ø§ØµÙ„ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø³Øª (Ø³Ù„Ú©ØªÙˆØ±Ù‡Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯)
            post_content_div = soup.find('div', class_='s19g0207-1') or \
                               soup.find('div', class_='_292iotee39Lmt0Q_h-B5N') or \
                               soup.find('div', class_='_1qeIAgB0cPwnLhDF9XHzvm') or \
                               soup.find('div', class_='_1qeIAgB0cPwnLhDF9XHzvm')
            
            if post_content_div:
                # ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù… Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø³Øª
                for a_tag in post_content_div.find_all('a', href=True):
                    href = a_tag['href']
                    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù† Ú©Ù‡ Ù„ÛŒÙ†Ú© Ø¨Ù‡ reddit.com Ù†ÛŒØ³Øª Ùˆ ÛŒÚ© Ù„ÛŒÙ†Ú© Ú©Ø§Ù…Ù„ HTTP/HTTPS Ø§Ø³Øª
                    if "reddit.com" not in href and href.startswith("http"):
                        logger.debug(f"[RedditSource - _fetch_and_parse_reddit_permalink] Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ø§Ø² Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª ÛŒØ§ÙØª Ø´Ø¯: {href}")
                        return href
                logger.warning(f"[RedditSource - _fetch_and_parse_reddit_permalink] Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª ÛŒØ§ÙØª Ù†Ø´Ø¯: {permalink_url}")
                return None
            else:
                logger.warning(f"âš ï¸ [RedditSource - _fetch_and_parse_reddit_permalink] Ú©Ø§Ù†ØªÛŒÙ†Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø³Øª Ø¯Ø± Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª '{permalink_url}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø³Ø§Ø®ØªØ§Ø± HTML Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯.")
                return None
        except Exception as e:
            logger.error(f"âŒ [RedditSource - _fetch_and_parse_reddit_permalink] Ø®Ø·Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø´Ø¯Ù‡ Ù‡Ù†Ú¯Ø§Ù… ØªØ¬Ø²ÛŒÙ‡ Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª {permalink_url}: {e}", exc_info=True)
            return None

    async def _normalize_post_data(self, session: aiohttp.ClientSession, entry: ET.Element, subreddit_name: str) -> Optional[Dict[str, Any]]:
        """
        Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ù¾Ø³Øª RSS Ø±Ø¯ÛŒØª Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ù¾Ø±ÙˆÚ˜Ù‡ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        raw_title = "Ù†Ø§Ù…Ø´Ø®Øµ" # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§
        try:
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            title_element = entry.find('atom:title', ns)
            content_element = entry.find('atom:content', ns)
            id_element = entry.find('atom:id', ns)

            if title_element is None or content_element is None or id_element is None:
                logger.debug(f"[RedditSource - _normalize_post_data] Ù¾Ø³Øª RSS Ù†Ø§Ù‚Øµ Ø¯Ø± Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name} ÛŒØ§ÙØª Ø´Ø¯ (Ø¹Ù†ÙˆØ§Ù†ØŒ Ù…Ø­ØªÙˆØ§ ÛŒØ§ ID Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª). Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                return None

            raw_title = title_element.text
            post_id = id_element.text
            logger.debug(f"[RedditSource - _normalize_post_data] Ø¯Ø± Ø­Ø§Ù„ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø³Øª Ø±Ø¯ÛŒØª: Ø¹Ù†ÙˆØ§Ù†='{raw_title}', ID='{post_id}'")
            
            soup = BeautifulSoup(content_element.text, 'html.parser')
            
            final_url = None
            
            # 1. ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ù„ÛŒÙ†Ú© Ù…Ø³ØªÙ‚ÛŒÙ… ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø³Øª (ØºÛŒØ± Ø§Ø² Ù„ÛŒÙ†Ú© [link] Ø§ØµÙ„ÛŒ)
            all_links_in_content = soup.find_all('a', href=True)
            for a_tag in all_links_in_content:
                href = a_tag['href']
                if "reddit.com" in href or not href.startswith("http"):
                    continue
                final_url = href # Ø§ÙˆÙ„ÛŒÙ† Ù„ÛŒÙ†Ú© Ù…Ø¹ØªØ¨Ø± Ø®Ø§Ø±Ø¬ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† final_url Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±
                logger.debug(f"[RedditSource - _normalize_post_data] Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø³Øª ÛŒØ§ÙØª Ø´Ø¯: {final_url}")
                break

            # 2. Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ù„ÛŒÙ†Ú© ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ… Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù„ÛŒÙ†Ú© [link] Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†
            if not final_url:
                link_tag = soup.find('a', string='[link]')
                if link_tag and 'href' in link_tag.attrs:
                    main_post_url = link_tag['href']
                    if "reddit.com" in main_post_url and "/comments/" in main_post_url:
                        logger.debug(f"[RedditSource - _normalize_post_data] Ù„ÛŒÙ†Ú© [link] Ø¨Ù‡ ÛŒÚ© Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª Ø§Ø´Ø§Ø±Ù‡ Ø¯Ø§Ø±Ø¯: {main_post_url}. Ø¯Ø± Ø­Ø§Ù„ ÙˆØ§Ú©Ø´ÛŒ Ù…Ø­ØªÙˆØ§...")
                        fetched_external_url = await self._fetch_and_parse_reddit_permalink(session, main_post_url)
                        if fetched_external_url:
                            final_url = fetched_external_url
                            logger.debug(f"[RedditSource - _normalize_post_data] Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ø§Ø² Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª ÙˆØ§Ú©Ø´ÛŒ Ø´Ø¯: {final_url}")
                        else:
                            logger.warning(f"âš ï¸ [RedditSource - _normalize_post_data] Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ø§Ø² Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª '{main_post_url}' Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯. Ø§Ø² Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ Ø±Ø¯ÛŒØª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
                            final_url = main_post_url # Fallback Ø¨Ù‡ Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª
                    else: # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© [link] ÛŒÚ© URL Ù…Ø³ØªÙ‚ÛŒÙ… ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø¨ÙˆØ¯
                        final_url = main_post_url
                        logger.debug(f"[RedditSource - _normalize_post_data] Ù„ÛŒÙ†Ú© [link] ÛŒÚ© URL Ù…Ø³ØªÙ‚ÛŒÙ… ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø§Ø³Øª: {final_url}")
                else:
                    logger.debug(f"[RedditSource - _normalize_post_data] Ù„ÛŒÙ†Ú© [link] Ø¯Ø± Ù¾Ø³Øª '{raw_title}' Ø§Ø² Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                    link_element = entry.find('atom:link', ns)
                    if link_element is not None and link_element.get('href'):
                        final_url = link_element.get('href')
                        logger.warning(f"âš ï¸ [RedditSource - _normalize_post_data] Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú© ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ…ÛŒ Ø¨Ø±Ø§ÛŒ '{raw_title}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§Ø² Ù„ÛŒÙ†Ú© RSS Ù¾Ø³Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯: {final_url}")
                    else:
                        logger.warning(f"âš ï¸ [RedditSource - _normalize_post_data] Ù‡ÛŒÚ† URL Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø³Øª '{raw_title}' Ø§Ø² Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name} ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                        return None
            
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² infer_store_from_game_data Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù†Ù‡Ø§ÛŒÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡
            # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø² utils.store_detector ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.
            detected_store = infer_store_from_game_data({"url": final_url, "title": raw_title})
            logger.debug(f"[RedditSource - _normalize_post_data] ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ '{raw_title}': {detected_store}")

            # --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª Ùˆ ØªØµÙˆÛŒØ± ---
            description_tag = soup.find('div', class_='md')
            description = description_tag.get_text(strip=True) if description_tag else ""

            image_tag = soup.find('img', src=True)
            image_url = image_tag['src'] if image_tag else None

            # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ù…Ø´ØªØ±Ú©
            clean_title = title_cleaner.clean_title_for_search(raw_title) # <--- ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
            
            if not clean_title:
                clean_title = raw_title.strip()
                if not clean_title:
                    logger.warning(f"âš ï¸ [RedditSource - _normalize_post_data] Ù¾Ø³Øª Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ú©Ø§Ù…Ù„Ø§Ù‹ Ø®Ø§Ù„ÛŒ Ø§Ø² RSS Ø±Ø¯ÛŒØª ({subreddit_name}) Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯. ID: {post_id}")
                    return None

            # ØªØ¹ÛŒÛŒÙ† is_free Ùˆ discount_text
            is_truly_free = False
            discount_text = None
            title_lower = raw_title.lower()

            if subreddit_name == 'FreeGameFindings':
                is_truly_free = True 
                logger.debug(f"â„¹ï¸ [RedditSource - _normalize_post_data] Ù¾Ø³Øª Ø§Ø² FreeGameFindings Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯: {raw_title}")
            elif "free" in title_lower or "100% off" in title_lower or "100% discount" in title_lower:
                is_truly_free = True
                logger.debug(f"[RedditSource - _normalize_post_data] Ù¾Ø³Øª '{raw_title}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ÛŒÚ¯Ø§Ù† (Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ) Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯.")
            elif "off" in title_lower: 
                is_truly_free = False 
                discount_match = re.search(r'(\d+% off)', title_lower)
                if discount_match:
                    discount_text = discount_match.group(1)
                else:
                    discount_text = "ØªØ®ÙÛŒÙ"
                logger.debug(f"[RedditSource - _normalize_post_data] Ù¾Ø³Øª '{raw_title}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± (Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ) Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯: {discount_text}")

            return {
                "title": clean_title,
                "store": detected_store, # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡
                "url": final_url, # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² URL Ù†Ù‡Ø§ÛŒÛŒ
                "image_url": image_url,
                "description": description,
                "id_in_db": post_id, # Ø´Ù†Ø§Ø³Ù‡ Ù¾Ø³Øª Ø±Ø¯ÛŒØª Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† id_in_db
                "subreddit": subreddit_name,
                "is_free": is_truly_free, # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† ÙÛŒÙ„Ø¯ is_free
                "discount_text": discount_text # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† ÙÛŒÙ„Ø¯ discount_text
            }
        except Exception as e:
            logger.error(f"âŒ [RedditSource - _normalize_post_data] Ø®Ø·Ø§ Ø¯Ø± Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø³Øª RSS Ø±Ø¯ÛŒØª Ø§Ø² Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name} (Ø¹Ù†ÙˆØ§Ù†: '{raw_title[:50]}...'): {e}", exc_info=True)
            return None

    def _parse_apphookup_weekly_deals(self, html_content: str, base_post_id: str) -> List[Dict[str, Any]]:
        """
        Ù…Ø­ØªÙˆØ§ÛŒ HTML Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ 'Weekly Deals' Ø§Ø² Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª AppHookup Ø±Ø§ ØªØ¬Ø²ÛŒÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        Ùˆ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¯Ø§Ø®Ù„ÛŒ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        found_items = []
        soup = BeautifulSoup(html_content, 'html.parser')
        logger.debug(f"[RedditSource - _parse_apphookup_weekly_deals] Ø¯Ø± Ø­Ø§Ù„ ØªØ¬Ø²ÛŒÙ‡ Ù…Ø­ØªÙˆØ§ÛŒ HTML Ø¨Ø±Ø§ÛŒ Ù¾Ø³Øª Weekly Deals (ID: {base_post_id}).")
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ URL Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ (ØªØ±ØªÛŒØ¨ Ù…Ù‡Ù… Ø§Ø³Øª: Ø®Ø§Øµâ€ŒØªØ±Ù‡Ø§ Ø§ÙˆÙ„) - Ø§ÛŒÙ†Ù‡Ø§ Ø¯ÛŒÚ¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
        # url_store_map_priority = [...]

        # Ø³Ù„Ú©ØªÙˆØ±Ù‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù„ÛŒØ³Øª Ø¯Ø± Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ AppHookup
        list_items = soup.find_all(['p', 'li'])
        logger.debug(f"[RedditSource - _parse_apphookup_weekly_deals] ØªØ¹Ø¯Ø§Ø¯ ØªÚ¯â€ŒÙ‡Ø§ÛŒ <p> ÛŒØ§ <li> ÛŒØ§ÙØª Ø´Ø¯Ù‡ Ø¯Ø± Weekly Deals: {len(list_items)}")

        for item_element in list_items:
            a_tag = item_element.find('a', href=True)
            if not a_tag:
                logger.debug("[RedditSource - _parse_apphookup_weekly_deals] ØªÚ¯ <a> Ø¯Ø± Ø¹Ù†ØµØ± Ù„ÛŒØ³Øª Weekly Deals ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                continue

            text_around_link = item_element.get_text().lower()
            item_title = a_tag.get_text().strip()
            item_url = a_tag['href']
            logger.debug(f"[RedditSource - _parse_apphookup_weekly_deals] Ø¢ÛŒØªÙ… Ø¯Ø§Ø®Ù„ÛŒ Weekly Deals: Ø¹Ù†ÙˆØ§Ù†='{item_title}', URL='{item_url}'")

            # Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø±Ø¯ÛŒØª Ø¯Ø§Ø®Ù„ÛŒ Ùˆ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ ØµØ±Ù Ù†Ø¸Ø± Ú©Ù†
            if "reddit.com" in item_url or not item_url.startswith("http"):
                logger.debug(f"[RedditSource - _parse_apphookup_weekly_deals] Ù„ÛŒÙ†Ú© Ø¯Ø§Ø®Ù„ÛŒ Ø±Ø¯ÛŒØª ÛŒØ§ Ù„ÛŒÙ†Ú© Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ '{item_title}'. Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                continue

            is_truly_free = False
            discount_text = None
            
            # ØªØ´Ø®ÛŒØµ "Ø±Ø§ÛŒÚ¯Ø§Ù†" ÛŒØ§ "ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø±"
            if "free" in text_around_link or "-> 0" in text_around_link or "--> 0" in text_around_link or "100% off" in text_around_link:
                if "off" in text_around_link and "100% off" not in text_around_link and "free" not in text_around_link:
                    is_truly_free = False # ØªØ®ÙÛŒÙ Ø¹Ø§Ø¯ÛŒ
                    discount_match = re.search(r'(\d+% off)', text_around_link)
                    if discount_match:
                        discount_text = discount_match.group(1)
                    else:
                        discount_text = "ØªØ®ÙÛŒÙ"
                    logger.debug(f"[RedditSource - _parse_apphookup_weekly_deals] Ø¢ÛŒØªÙ… '{item_title}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± (Ù…ØªÙ†: '{text_around_link}') Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯.")
                else:
                    is_truly_free = True # ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§ÛŒÚ¯Ø§Ù†
                    logger.debug(f"[RedditSource - _parse_apphookup_weekly_deals] Ø¢ÛŒØªÙ… '{item_title}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ÛŒÚ¯Ø§Ù† (Ù…ØªÙ†: '{text_around_link}') Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯.")
            elif "off" in text_around_link: # Ø§Ú¯Ø± ÙÙ‚Ø· "off" Ø¨ÙˆØ¯ Ùˆ "free" Ù†Ø¨ÙˆØ¯
                is_truly_free = False # Ø§ÛŒÙ† ÛŒÚ© ØªØ®ÙÛŒÙ Ø§Ø³ØªØŒ Ù†Ù‡ Ø±Ø§ÛŒÚ¯Ø§Ù†
                discount_match = re.search(r'(\d+% off)', text_around_link)
                if discount_match:
                    discount_text = discount_match.group(1)
                else:
                    discount_text = "ØªØ®ÙÛŒÙ"
                logger.debug(f"[RedditSource - _parse_apphookup_weekly_deals] Ø¢ÛŒØªÙ… '{item_title}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± (Ù…ØªÙ†: '{text_around_link}') Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯.")

            if is_truly_free or (not is_truly_free and discount_text): # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù‡Ù… Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ù‡Ù… ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø±
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² infer_store_from_game_data Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù†Ù‡Ø§ÛŒÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡
                detected_store = infer_store_from_game_data({"url": item_url, "title": item_title})
                logger.debug(f"[RedditSource - _parse_apphookup_weekly_deals] ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ '{item_title}': {detected_store}")
                
                item_description = item_element.get_text(separator=' ', strip=True)
                item_description = item_description.replace(item_title, '').replace(item_url, '').strip()
                if len(item_description) < 20: 
                    item_description = item_title
                logger.debug(f"[RedditSource - _parse_apphookup_weekly_deals] ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø±Ø§ÛŒ '{item_title}': {item_description[:50]}...")

                item_image_tag = item_element.find('img', src=True)
                item_image_url = item_image_tag['src'] if item_image_tag else None
                logger.debug(f"[RedditSource - _parse_apphookup_weekly_deals] ØªØµÙˆÛŒØ± Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø±Ø§ÛŒ '{item_title}': {item_image_url}")
                
                if item_title:
                    found_items.append({
                        "title": title_cleaner.clean_title_for_search(item_title), # <--- ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
                        "store": detected_store, # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡
                        "url": item_url,
                        "image_url": item_image_url,
                        "description": item_description,
                        "id_in_db": self._generate_unique_id(base_post_id, item_url),
                        "subreddit": "AppHookup",
                        "is_free": is_truly_free, # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† ÙÛŒÙ„Ø¯ is_free
                        "discount_text": discount_text # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† ÙÛŒÙ„Ø¯ discount_text
                    })
                    if is_truly_free:
                        logger.info(f"âœ… Ø¢ÛŒØªÙ… Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¯Ø§Ø®Ù„ÛŒ Ø§Ø² Ù„ÛŒØ³Øª 'Weekly Deals' (AppHookup) ÛŒØ§ÙØª Ø´Ø¯: {item_title} (ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {detected_store})")
                    else:
                        logger.info(f"ğŸ” Ø¢ÛŒØªÙ… ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ø§Ø² Ù„ÛŒØ³Øª 'Weekly Deals' (AppHookup) ÛŒØ§ÙØª Ø´Ø¯: {item_title} (ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {detected_store}, ØªØ®ÙÛŒÙ: {discount_text})")
                else:
                    logger.warning(f"âš ï¸ [RedditSource - _parse_apphookup_weekly_deals] Ø¢ÛŒØªÙ… Ø±Ø§ÛŒÚ¯Ø§Ù†/ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ø®Ø§Ù„ÛŒ Ø§Ø² AppHookup Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯. URL: {item_url}")
            else:
                logger.debug(f"ğŸ” [RedditSource - _parse_apphookup_weekly_deals] Ø¢ÛŒØªÙ… Ø¯Ø§Ø®Ù„ÛŒ '{item_title}' Ø§Ø² AppHookup Ø±Ø§ÛŒÚ¯Ø§Ù†/ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ù†Ø¨ÙˆØ¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                
        return found_items

    async def fetch_free_games(self) -> List[Dict[str, Any]]:
        logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø² ÙÛŒØ¯ RSS Ø±Ø¯ÛŒØª...")
        free_games_list = []
        processed_ids = set()

        async with aiohttp.ClientSession() as session:
            for subreddit_name, url in self.rss_urls.items():
                logger.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³Ú©Ø§Ù† ÙÛŒØ¯ RSS: {url} (Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª: {subreddit_name})...")
                rss_content = await self._fetch_with_retry(session, url)
                if not rss_content:
                    logger.error(f"âŒ ÙˆØ§Ú©Ø´ÛŒ ÙÛŒØ¯ RSS Ø§Ø² {url} Ø¨Ø§ Ø´Ú©Ø³Øª Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯. Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ù‡ Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª Ø¨Ø¹Ø¯ÛŒ.")
                    continue

                try:
                    root = ET.fromstring(rss_content)
                    logger.debug(f"ÙÛŒØ¯ RSS Ø¨Ø±Ø§ÛŒ {subreddit_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªØ¬Ø²ÛŒÙ‡ Ø´Ø¯.")
                except ET.ParseError as e:
                    logger.error(f"âŒ Ø®Ø·Ø§ÛŒ ØªØ¬Ø²ÛŒÙ‡ Ù…Ø­ØªÙˆØ§ÛŒ ÙÛŒØ¯ RSS Ø§Ø² {url}: {e}", exc_info=True)
                    continue
                
                ns = {'atom': 'http://www.w3.org/2005/Atom'}
                entries = root.findall('atom:entry', ns)
                logger.debug(f"ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡ Ø¯Ø± ÙÛŒØ¯ RSS Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name}: {len(entries)}")

                for entry in entries:
                    title_element = entry.find('atom:title', ns)
                    content_element = entry.find('atom:content', ns)
                    id_element = entry.find('atom:id', ns)

                    if title_element is None or content_element is None or id_element is None:
                        logger.debug(f"Ù¾Ø³Øª RSS Ù†Ø§Ù‚Øµ Ø¯Ø± Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name} ÛŒØ§ÙØª Ø´Ø¯ (Ø¹Ù†ÙˆØ§Ù†ØŒ Ù…Ø­ØªÙˆØ§ ÛŒØ§ ID Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª). Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                        continue

                    raw_title = title_element.text
                    post_id = id_element.text

                    is_truly_free_post = False
                    discount_text_post = None
                    title_lower = raw_title.lower()
                    
                    # Ù…Ù†Ø·Ù‚ ØªØ´Ø®ÛŒØµ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¨ÙˆØ¯Ù†/ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ø¨ÙˆØ¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¯Ø± Ø¹Ù†ÙˆØ§Ù†
                    if subreddit_name == 'FreeGameFindings':
                        is_truly_free_post = True 
                        logger.debug(f"â„¹ï¸ [RedditSource - fetch_free_games] Ù¾Ø³Øª Ø§Ø² FreeGameFindings Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯: {raw_title}")
                    elif "free" in title_lower or "100% off" in title_lower or "100% discount" in title_lower:
                        is_truly_free_post = True
                        logger.debug(f"[RedditSource - fetch_free_games] Ù¾Ø³Øª '{raw_title}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ÛŒÚ¯Ø§Ù† (Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ) Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯.")
                    elif "off" in title_lower: 
                        is_truly_free_post = False 
                        discount_match = re.search(r'(\d+% off)', title_lower)
                        if discount_match:
                            discount_text_post = discount_match.group(1)
                        else:
                            discount_text_post = "ØªØ®ÙÛŒÙ"
                        logger.debug(f"[RedditSource - fetch_free_games] Ù¾Ø³Øª '{raw_title}' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± (Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ) Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯: {discount_text_post}")

                    # Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ AppHookup weekly deals
                    if subreddit_name == 'AppHookup' and ("weekly" in title_lower and ("deals post" in title_lower or "app deals post" in title_lower or "game deals post" in title_lower)):
                        logger.info(f"ğŸ” [RedditSource - fetch_free_games] Ù¾Ø³Øª 'Weekly Deals' Ø§Ø² AppHookup Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯: {raw_title}. Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ...")
                        weekly_items = self._parse_apphookup_weekly_deals(content_element.text, post_id)
                        for item in weekly_items:
                            if item['id_in_db'] not in processed_ids:
                                free_games_list.append(item)
                                processed_ids.add(item['id_in_db'])
                                # Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø¯Ø± _parse_apphookup_weekly_deals Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
                            else:
                                logger.debug(f"â„¹ï¸ [RedditSource - fetch_free_games] Ø¢ÛŒØªÙ… Ø¯Ø§Ø®Ù„ÛŒ '{item['title']}' Ø§Ø² Weekly Deals Ù‚Ø¨Ù„Ø§Ù‹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯.")
                        continue # Ù¾Ø³ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒØŒ Ø¨Ù‡ Ù¾Ø³Øª Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±ÙˆÛŒØ¯

                    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¹Ø§Ø¯ÛŒ (ØºÛŒØ± Ø§Ø² Weekly Deals)
                    if is_truly_free_post or (not is_truly_free_post and discount_text_post):
                        normalized_game = await self._normalize_post_data(session, entry, subreddit_name)
                        if normalized_game:
                            normalized_game['is_free'] = is_truly_free_post
                            normalized_game['discount_text'] = discount_text_post

                            if normalized_game['title'].strip():
                                if normalized_game['id_in_db'] not in processed_ids:
                                    free_games_list.append(normalized_game)
                                    processed_ids.add(normalized_game['id_in_db'])
                                    if normalized_game['is_free']:
                                        logger.info(f"âœ… [RedditSource - fetch_free_games] Ù¾Ø³Øª Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø² RSS Ø±Ø¯ÛŒØª ({normalized_game['subreddit']}) ÛŒØ§ÙØª Ø´Ø¯: {normalized_game['title']} (ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {normalized_game['store']})")
                                    else:
                                        logger.info(f"ğŸ” [RedditSource - fetch_free_games] Ù¾Ø³Øª ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ø§Ø² RSS Ø±Ø¯ÛŒØª ({normalized_game['subreddit']}) ÛŒØ§ÙØª Ø´Ø¯: {normalized_game['title']} (ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {normalized_game['store']}, ØªØ®ÙÛŒÙ: {normalized_game['discount_text']})")
                                else:
                                    logger.debug(f"â„¹ï¸ [RedditSource - fetch_free_games] Ù¾Ø³Øª '{raw_title}' Ø§Ø² {subreddit_name} Ø§Ø² Ù‚Ø¨Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯.")
                            else:
                                logger.warning(f"âš ï¸ [RedditSource - fetch_free_games] Ù¾Ø³Øª Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù†/ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ø®Ø§Ù„ÛŒ Ø§Ø² RSS Ø±Ø¯ÛŒØª ({subreddit_name}) Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯. ID: {normalized_game['id_in_db']}")
                                continue 
                        else:
                            logger.debug(f"â„¹ï¸ [RedditSource - fetch_free_games] Ù¾Ø³Øª '{raw_title}' Ø§Ø² {subreddit_name} Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ø´Ø¯. Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                    else:
                        logger.debug(f"ğŸ” [RedditSource - fetch_free_games] Ù¾Ø³Øª '{raw_title}' Ø§Ø² {subreddit_name} Ø´Ø±Ø§ÛŒØ· 'Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù†' ÛŒØ§ 'ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø±' Ø±Ø§ Ù†Ø¯Ø§Ø´Øª Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")

        if not free_games_list:
            logger.info("â„¹ï¸ [RedditSource - fetch_free_games] Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ù¾Ø³Øª Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¬Ø¯ÛŒØ¯ÛŒ Ø¯Ø± ÙÛŒØ¯Ù‡Ø§ÛŒ RSS Ø±Ø¯ÛŒØª ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            
        return free_games_list
