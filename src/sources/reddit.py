import logging
import asyncio
from typing import List, Dict, Any, Optional
import aiohttp
import xml.etree.ElementTree as ET
import re
from bs4 import BeautifulSoup
import hashlib
from utils import clean_title_for_search # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† ØªØ§Ø¨Ø¹ ØªÙ…ÛŒØ²Ú©Ù†Ù†Ø¯Ù‡ Ù…Ø´ØªØ±Ú©

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# ØªØ§Ø¨Ø¹ _clean_title_for_search_common Ø­Ø°Ù Ø´Ø¯ Ùˆ Ø§Ø² utils.clean_title_for_search Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

class RedditSource:
    def __init__(self):
        self.subreddits = [
            'GameDeals',
            'FreeGameFindings',
            'googleplaydeals',
            'AppHookup'
        ]
        self.rss_urls = {sub: f"https://www.reddit.com/r/{sub}/new/.rss" for sub in self.subreddits}
        logger.info("Ù†Ù…ÙˆÙ†Ù‡ RedditSource (Ù†Ø³Ø®Ù‡ RSS Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯.")

    @staticmethod
    def _generate_unique_id(base_id: str, item_url: str) -> str:
        combined_string = f"{base_id}-{item_url}"
        return hashlib.sha256(combined_string.encode()).hexdigest()

    async def _fetch_and_parse_reddit_permalink(self, session: aiohttp.ClientSession, permalink_url: str) -> Optional[str]:
        """
        ÛŒÚ© Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª Ø±Ø§ ÙˆØ§Ú©Ø´ÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ Ø§ÙˆÙ„ÛŒÙ† Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ù…Ø¹ØªØ¨Ø± Ø±Ø§ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        try:
            logger.info(f"Ø¯Ø± Ø­Ø§Ù„ ÙˆØ§Ú©Ø´ÛŒ Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ: {permalink_url}")
            async with session.get(permalink_url, headers={'User-agent': 'GameBeaconBot/1.0'}) as response:
                response.raise_for_status()
                html_content = await response.text()
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† div Ø§ØµÙ„ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø³Øª
                # Ø§ÛŒÙ† Ø³Ù„Ú©ØªÙˆØ±Ù‡Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ Ø§Ú¯Ø± Ø±Ø¯ÛŒØª UI Ø®ÙˆØ¯ Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡Ø¯.
                post_content_div = soup.find('div', class_='s19g0207-1') 
                if not post_content_div:
                    post_content_div = soup.find('div', class_='_292iotee39Lmt0Q_h-B5N') 
                if not post_content_div:
                    post_content_div = soup.find('div', class_='_1qeIAgB0cPwnLhDF9XHzvm') 
                
                if post_content_div:
                    # ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù… Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø³Øª
                    for a_tag in post_content_div.find_all('a', href=True):
                        href = a_tag['href']
                        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù† Ú©Ù‡ Ù„ÛŒÙ†Ú© Ø¨Ù‡ reddit.com Ù†ÛŒØ³Øª Ùˆ ÛŒÚ© Ù„ÛŒÙ†Ú© Ú©Ø§Ù…Ù„ HTTP/HTTPS Ø§Ø³Øª
                        if "reddit.com" not in href and href.startswith("http"):
                            logger.info(f"Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ø§Ø² Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª ÛŒØ§ÙØª Ø´Ø¯: {href}")
                            return href
                logger.warning(f"Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¯Ø± Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª ÛŒØ§ÙØª Ù†Ø´Ø¯: {permalink_url}")
                return None
        except aiohttp.ClientError as e:
            logger.error(f"Ø®Ø·Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ù‡Ù†Ú¯Ø§Ù… ÙˆØ§Ú©Ø´ÛŒ Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª {permalink_url}: {e}")
            return None
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø´Ø¯Ù‡ Ù‡Ù†Ú¯Ø§Ù… ØªØ¬Ø²ÛŒÙ‡ Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª {permalink_url}: {e}", exc_info=True)
            return None

    async def _normalize_post_data(self, session: aiohttp.ClientSession, entry: ET.Element, subreddit_name: str) -> Optional[Dict[str, Any]]:
        try:
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            title_element = entry.find('atom:title', ns)
            content_element = entry.find('atom:content', ns)
            id_element = entry.find('atom:id', ns)

            if title_element is None or content_element is None or id_element is None:
                logger.debug(f"Ù¾Ø³Øª RSS Ù†Ø§Ù‚Øµ Ø¯Ø± Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name} ÛŒØ§ÙØª Ø´Ø¯ (Ø¹Ù†ÙˆØ§Ù†ØŒ Ù…Ø­ØªÙˆØ§ ÛŒØ§ ID Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª).")
                return None

            raw_title = title_element.text
            post_id = id_element.text
            
            soup = BeautifulSoup(content_element.text, 'html.parser')
            
            # --- Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ URL Ùˆ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ ---
            final_url = None
            detected_store = 'other' # Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶

            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ URL Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ (ØªØ±ØªÛŒØ¨ Ù…Ù‡Ù… Ø§Ø³Øª: Ø®Ø§Øµâ€ŒØªØ±Ù‡Ø§ Ø§ÙˆÙ„)
            url_store_map_priority = [
                ("apps.apple.com", "ios app store"),
                ("play.google.com", "google play"),
                ("store.steampowered.com", "steam"),
                # Epic Games desktop/mobile links - order matters for specificity
                ("epicgames.com/store/p/.*-android-", "google play"), 
                ("epicgames.com/store/p/.*-ios-", "ios app store"),
                ("epicgames.com/store/p/", "epic games"), # General Epic Desktop, if not mobile
                ("gog.com", "gog"),
                ("xbox.com", "xbox"),
                ("itch.io", "itch.io"),
                ("indiegala.com", "indiegala"),
                ("onstove.com", "stove"),
            ]

            # 1. ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ù„ÛŒÙ†Ú© Ù…Ø³ØªÙ‚ÛŒÙ… ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø³Øª (ØºÛŒØ± Ø§Ø² Ù„ÛŒÙ†Ú© [link] Ø§ØµÙ„ÛŒ)
            # Ø§ÛŒÙ† Ø´Ø§Ù…Ù„ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ú©Ù‡ Ø¯Ø± Ù…ØªÙ† Ù¾Ø³Øª Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
            all_links_in_content = soup.find_all('a', href=True)
            for a_tag in all_links_in_content:
                href = a_tag['href']
                # Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø±Ø¯ÛŒØª Ø¯Ø§Ø®Ù„ÛŒ Ùˆ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ ØµØ±Ù Ù†Ø¸Ø± Ú©Ù†
                if "reddit.com" in href or not href.startswith("http"):
                    continue
                
                for pattern, store_name in url_store_map_priority:
                    if re.search(pattern, href, re.IGNORECASE):
                        final_url = href
                        detected_store = store_name
                        break # Ø§ÙˆÙ„ÛŒÙ† ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ØªØ± Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ÛŒÙ…
                if final_url: # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú©ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯ØŒ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø®Ø§Ø±Ø¬ Ø´Ùˆ
                    break

            # 2. Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ù„ÛŒÙ†Ú© ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ… Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù„ÛŒÙ†Ú© [link] Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†
            if not final_url:
                link_tag = soup.find('a', string='[link]')
                if link_tag and 'href' in link_tag.attrs:
                    main_post_url = link_tag['href']
                    # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© [link] ÛŒÚ© Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª Ø¨Ø§Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ ÙˆØ§Ú©Ø´ÛŒ Ùˆ ØªØ¬Ø²ÛŒÙ‡ Ú©Ù†
                    if "reddit.com" in main_post_url and "/comments/" in main_post_url:
                        fetched_external_url = await self._fetch_and_parse_reddit_permalink(session, main_post_url)
                        if fetched_external_url:
                            final_url = fetched_external_url
                            # Ù¾Ø³ Ø§Ø² ÙˆØ§Ú©Ø´ÛŒØŒ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ø² URL Ø¬Ø¯ÛŒØ¯ Ø­Ø¯Ø³ Ø¨Ø²Ù†
                            for pattern, store_name in url_store_map_priority:
                                if re.search(pattern, final_url, re.IGNORECASE):
                                    detected_store = store_name
                                    break
                        else:
                            logger.warning(f"Ù„ÛŒÙ†Ú© Ø®Ø§Ø±Ø¬ÛŒ Ø§Ø² Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª '{main_post_url}' Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯. Ø§Ø² Ù„ÛŒÙ†Ú© Ø§ØµÙ„ÛŒ Ø±Ø¯ÛŒØª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
                            final_url = main_post_url # Fallback Ø¨Ù‡ Ù„ÛŒÙ†Ú© Ø¯Ø§Ø¦Ù…ÛŒ Ø±Ø¯ÛŒØª
                    else:
                        final_url = main_post_url
                        # ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø±Ø§ Ø§Ø² Ù„ÛŒÙ†Ú© [link] Ø§ØµÙ„ÛŒ Ø­Ø¯Ø³ Ø¨Ø²Ù†
                        for pattern, store_name in url_store_map_priority:
                            if re.search(pattern, final_url, re.IGNORECASE):
                                detected_store = store_name
                                break
                else:
                    logger.debug(f"Ù„ÛŒÙ†Ú© [link] Ø¯Ø± Ù¾Ø³Øª '{raw_title}' Ø§Ø² Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                    return None # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú©ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù¾Ø³Øª Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ø¨Ú¯ÛŒØ±

            # 3. Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² URL Ù…Ø¹ØªØ¨Ø±ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø² URL Ø§ØµÙ„ÛŒ Ù¾Ø³Øª RSS Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† (Ú©Ù…ØªØ±ÛŒÙ† Ø§ÙˆÙ„ÙˆÛŒØª)
            if not final_url:
                # Ø§ÛŒÙ† URL Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¨Ù‡ Ø®ÙˆØ¯ Ù¾Ø³Øª Ø±Ø¯ÛŒØª Ø§Ø´Ø§Ø±Ù‡ Ø¯Ø§Ø±Ø¯ØŒ Ø§Ù…Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¢Ø®Ø±ÛŒÙ† Ø±Ø§Ù‡ Ø­Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
                # Ø§ÛŒÙ† Ø­Ø§Ù„Øª Ù†Ø¨Ø§ÛŒØ¯ Ø²ÛŒØ§Ø¯ Ù¾ÛŒØ´ Ø¨ÛŒØ§ÛŒØ¯ Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© [link] Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÙˆØ¯.
                link_element = entry.find('atom:link', ns)
                if link_element is not None and link_element.get('href'):
                    final_url = link_element.get('href')
                    logger.warning(f"Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú© ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ…ÛŒ Ø¨Ø±Ø§ÛŒ '{raw_title}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§Ø² Ù„ÛŒÙ†Ú© RSS Ù¾Ø³Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯: {final_url}")
                else:
                    logger.warning(f"Ù‡ÛŒÚ† URL Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø³Øª '{raw_title}' Ø§Ø² Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name} ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                    return None
            
            # --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª Ùˆ ØªØµÙˆÛŒØ± ---
            description_tag = soup.find('div', class_='md')
            description = description_tag.get_text(strip=True) if description_tag else ""

            image_tag = soup.find('img', src=True)
            image_url = image_tag['src'] if image_tag else None

            # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ù…Ø´ØªØ±Ú©
            clean_title = clean_title_for_search(raw_title)
            
            if not clean_title:
                clean_title = raw_title.strip()
                if not clean_title:
                    logger.warning(f"âš ï¸ Ù¾Ø³Øª Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ú©Ø§Ù…Ù„Ø§Ù‹ Ø®Ø§Ù„ÛŒ Ø§Ø² RSS Ø±Ø¯ÛŒØª ({subreddit_name}) Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯. ID: {post_id}")
                    return None

            return {
                "title": clean_title,
                "store": detected_store, # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡
                "url": final_url, # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² URL Ù†Ù‡Ø§ÛŒÛŒ
                "image_url": image_url,
                "description": description,
                "id_in_db": post_id, # Ø´Ù†Ø§Ø³Ù‡ Ù¾Ø³Øª Ø±Ø¯ÛŒØª Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† id_in_db
                "subreddit": subreddit_name
            }
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø³Øª RSS Ø±Ø¯ÛŒØª Ø§Ø² Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name}: {e}", exc_info=True)
            return None

    def _parse_apphookup_weekly_deals(self, html_content: str, base_post_id: str) -> List[Dict[str, Any]]:
        found_items = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        url_store_map_priority = [
            ("apps.apple.com", "ios app store"),
            ("play.google.com", "google play"),
            ("store.steampowered.com", "steam"),
            ("epicgames.com/store/p/.*-android-", "google play"), 
            ("epicgames.com/store/p/.*-ios-", "ios app store"),
            ("epicgames.com/store/p/", "epic games"),
            ("gog.com", "gog"),
            ("xbox.com", "xbox"),
            ("itch.io", "itch.io"),
            ("indiegala.com", "indiegala"),
            ("onstove.com", "stove"),
        ]

        for a_tag in soup.find_all('a', href=True):
            parent_text_element = a_tag.find_parent(['p', 'li'])
            if parent_text_element:
                text_around_link = parent_text_element.get_text().lower()
                item_title = a_tag.get_text().strip()
                item_url = a_tag['href']

                is_free = False
                # ØªØ´Ø®ÛŒØµ "Ø±Ø§ÛŒÚ¯Ø§Ù†" ÛŒØ§ "ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø±"
                if "free" in text_around_link or "-> 0" in text_around_link or "--> 0" in text_around_link:
                    if "off" in text_around_link and "100% off" not in text_around_link and "free" not in text_around_link:
                        is_free = False # ØªØ®ÙÛŒÙ Ø¹Ø§Ø¯ÛŒ
                    else:
                        is_free = True # ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§ÛŒÚ¯Ø§Ù†
                elif "off" in text_around_link: # Ø§Ú¯Ø± ÙÙ‚Ø· "off" Ø¨ÙˆØ¯ Ùˆ "free" Ù†Ø¨ÙˆØ¯
                    is_free = False # Ø§ÛŒÙ† ÛŒÚ© ØªØ®ÙÛŒÙ Ø§Ø³ØªØŒ Ù†Ù‡ Ø±Ø§ÛŒÚ¯Ø§Ù†

                if is_free: # ÙÙ‚Ø· Ø¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                    store = "other"
                    # 1. ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø­Ø¯Ø³ Ø²Ø¯Ù† Ø§Ø² URL Ø§ØµÙ„ÛŒ
                    for pattern, store_name in url_store_map_priority:
                        if re.search(pattern, item_url, re.IGNORECASE):
                            store = store_name
                            break
                    
                    item_description = parent_text_element.get_text(separator=' ', strip=True)
                    item_description = item_description.replace(item_title, '').replace(item_url, '').strip()
                    if len(item_description) < 20:
                        item_description = item_title

                    item_image_tag = parent_text_element.find('img', src=True)
                    item_image_url = item_image_tag['src'] if item_image_tag else None
                    
                    if item_title:
                        found_items.append({
                            "title": clean_title_for_search(item_title), # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ø¹Ù†ÙˆØ§Ù† Ø¢ÛŒØªÙ… Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø§ ØªØ§Ø¨Ø¹ Ù…Ø´ØªØ±Ú©
                            "store": store,
                            "url": item_url,
                            "image_url": item_image_url,
                            "description": item_description,
                            "id_in_db": self._generate_unique_id(base_post_id, item_url),
                            "subreddit": "AppHookup"
                        })
                        logger.debug(f"âœ… Ø¢ÛŒØªÙ… Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¯Ø§Ø®Ù„ÛŒ Ø§Ø² AppHookup ÛŒØ§ÙØª Ø´Ø¯: {item_title} (URL: {item_url})")
                    else:
                        logger.warning(f"âš ï¸ Ø¢ÛŒØªÙ… Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ø®Ø§Ù„ÛŒ Ø§Ø² AppHookup Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯. URL: {item_url}")
                else:
                    logger.debug(f"ğŸ” Ø¢ÛŒØªÙ… Ø¯Ø§Ø®Ù„ÛŒ '{item_title}' Ø§Ø² AppHookup Ø±Ø§ÛŒÚ¯Ø§Ù† Ù†Ø¨ÙˆØ¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
            
        return found_items

    async def fetch_free_games(self) -> List[Dict[str, Any]]:
        logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø² ÙÛŒØ¯ RSS Ø±Ø¯ÛŒØª...")
        free_games_list = []
        processed_ids = set()

        try:
            for subreddit_name, url in self.rss_urls.items():
                logger.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³Ú©Ø§Ù† ÙÛŒØ¯ RSS: {url} (Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª: {subreddit_name})...")
                async with aiohttp.ClientSession() as session:
                    headers = {'User-agent': 'GameBeaconBot/1.0'}
                    async with session.get(url, headers=headers) as response:
                        if response.status != 200:
                            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ÙÛŒØ¯ {url}: Status {response.status}")
                            continue
                        
                        rss_content = await response.text()
                        root = ET.fromstring(rss_content)
                        
                        ns = {'atom': 'http://www.w3.org/2005/Atom'}
                        for entry in root.findall('atom:entry', ns):
                            title_element = entry.find('atom:title', ns)
                            content_element = entry.find('atom:content', ns)
                            id_element = entry.find('atom:id', ns)

                            if title_element is None or content_element is None or id_element is None:
                                logger.debug(f"Ù¾Ø³Øª RSS Ù†Ø§Ù‚Øµ Ø¯Ø± Ø³Ø§Ø¨â€ŒØ±Ø¯ÛŒØª {subreddit_name} (Ø¹Ù†ÙˆØ§Ù†ØŒ Ù…Ø­ØªÙˆØ§ ÛŒØ§ ID Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª).")
                                continue

                            title_lower = title_element.text.lower()
                            post_id = id_element.text

                            is_truly_free = False
                            is_discounted_but_not_free = False
                            
                            # Ù…Ù†Ø·Ù‚ ØªØ´Ø®ÛŒØµ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¨ÙˆØ¯Ù†/ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ø¨ÙˆØ¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¯Ø± Ø¹Ù†ÙˆØ§Ù†
                            if "free" in title_lower or "100% off" in title_lower or "100% discount" in title_lower:
                                is_truly_free = True
                            elif "off" in title_lower: # Ø§Ú¯Ø± Ú©Ù„Ù…Ù‡ "off" Ø¨ÙˆØ¯ ÙˆÙ„ÛŒ "free" ÛŒØ§ "100% off" Ù†Ø¨ÙˆØ¯
                                is_discounted_but_not_free = True

                            # Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ AppHookup weekly deals
                            if subreddit_name == 'AppHookup' and ("weekly" in title_lower and ("deals post" in title_lower or "app deals post" in title_lower or "game deals post" in title_lower)):
                                logger.info(f"ğŸ” Ù¾Ø³Øª 'Weekly Deals' Ø§Ø² AppHookup Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯: {title_element.text}. Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ...")
                                weekly_items = self._parse_apphookup_weekly_deals(content_element.text, post_id)
                                for item in weekly_items:
                                    if item['id_in_db'] not in processed_ids:
                                        free_games_list.append(item)
                                        processed_ids.add(item['id_in_db'])
                                        logger.info(f"âœ… Ø¢ÛŒØªÙ… Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø² Ù„ÛŒØ³Øª 'Weekly Deals' ({item['subreddit']}) ÛŒØ§ÙØª Ø´Ø¯: {item['title']} (ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {item['store']})")
                                continue # Ù¾Ø³ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒØŒ Ø¨Ù‡ Ù¾Ø³Øª Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±ÙˆÛŒØ¯

                            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¹Ø§Ø¯ÛŒ (ØºÛŒØ± Ø§Ø² Weekly Deals)
                            if is_truly_free or is_discounted_but_not_free:
                                normalized_game = await self._normalize_post_data(session, entry, subreddit_name)
                                if normalized_game:
                                    if is_discounted_but_not_free:
                                        normalized_game['store'] = "Not Free (Discount)" # Ø§Ø®ØªØµØ§Øµ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ÙˆÛŒÚ˜Ù‡
                                        logger.info(f"âš ï¸ Ù¾Ø³Øª ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø± Ø§Ø² RSS Ø±Ø¯ÛŒØª ({normalized_game['subreddit']}) ÛŒØ§ÙØª Ø´Ø¯: {normalized_game['title']} (ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {normalized_game['store']})")
                                    elif normalized_game['title'].strip(): # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø®Ø§Ù„ÛŒ Ù†Ø¨ÙˆØ¯Ù† Ø¹Ù†ÙˆØ§Ù† Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø±Ø§ÛŒÚ¯Ø§Ù†
                                        logger.info(f"âœ… Ù¾Ø³Øª Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø² RSS Ø±Ø¯ÛŒØª ({normalized_game['subreddit']}) ÛŒØ§ÙØª Ø´Ø¯: {normalized_game['title']} (ÙØ±ÙˆØ´Ú¯Ø§Ù‡: {normalized_game['store']})")
                                    else:
                                        logger.warning(f"âš ï¸ Ù¾Ø³Øª Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ø®Ø§Ù„ÛŒ Ø§Ø² RSS Ø±Ø¯ÛŒØª ({subreddit_name}) Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯. ID: {normalized_game['id_in_db']}")
                                        continue # Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ØŒ Ø­ØªÛŒ Ø§Ú¯Ø± Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ØŒ Ø±Ø¯ Ú©Ù†

                                    if normalized_game['id_in_db'] not in processed_ids:
                                        free_games_list.append(normalized_game)
                                        processed_ids.add(normalized_game['id_in_db'])
                                    else:
                                        logger.debug(f"â„¹ï¸ Ù¾Ø³Øª '{title_element.text}' Ø§Ø² {subreddit_name} Ø§Ø² Ù‚Ø¨Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯.")
                                else:
                                    logger.debug(f"â„¹ï¸ Ù¾Ø³Øª '{title_element.text}' Ø§Ø² {subreddit_name} Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ø´Ø¯.")
                            else:
                                logger.debug(f"ğŸ” Ù¾Ø³Øª '{title_element.text}' Ø§Ø² {subreddit_name} Ø´Ø±Ø§ÛŒØ· 'Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù†' ÛŒØ§ 'ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø±' Ø±Ø§ Ù†Ø¯Ø§Ø´Øª Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")

        except Exception as e:
            logger.critical(f"ğŸ”¥ ÛŒÚ© Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø´Ø¯Ù‡ Ø¯Ø± Ù…Ø§Ú˜ÙˆÙ„ Reddit (RSS) Ø±Ø® Ø¯Ø§Ø¯: {e}", exc_info=True)
            
        if not free_games_list:
            logger.info("â„¹ï¸ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ù¾Ø³Øª Ø¨Ø§Ø²ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¬Ø¯ÛŒØ¯ÛŒ Ø¯Ø± ÙÛŒØ¯Ù‡Ø§ÛŒ RSS Ø±Ø¯ÛŒØª ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            
        return free_games_list
